{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e762f904",
   "metadata": {},
   "source": [
    "### This is the code for the web scraping functions using pygooglenews and other packages to scrape Google News according to a matrix of factors including author name, key words found in the titles of the peer-reviewed publications, DOI's and peer-reviewed journal names. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300dc44",
   "metadata": {},
   "source": [
    "### Import pygooglenews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f96260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import difflib as difflib\n",
    "\n",
    "from pygooglenews import GoogleNews\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "\n",
    "gn = GoogleNews() # now global mode? can specify country = US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70381b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c86488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd7f2bfa",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb4ae337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df):\n",
    "    \"\"\"Main function which accepts a CSV file as input. Invokes (i) all scraping functions, \n",
    "    (ii) function which consolidates a list of relevant keywords. Returns a csv file containing\n",
    "    academic journal article title, author, and DOI appended to related news \n",
    "    articles and their corresponding URLS\"\"\"\n",
    "    # must be in same repository that the jupyter notebook is located in\n",
    "    # df = pd.read_csv(\"UseCase1_Data.csv\")\n",
    "    \n",
    "    df = df.dropna(how ='all') # drop last n rows because google news functions \n",
    "    #won't work with NaN or NA values, we need to add an ifelse/judgement to each function. We only can search for strings\n",
    "    df.dropna(how='all', axis=1, inplace=True)\n",
    "    \n",
    "    # Matching steps\n",
    "    # author_cols = [col for col in df.columns if 'author' in col]+[col for col in df.columns if 'Author' in col]\n",
    "    #title_cols = [col for col in df.columns if 'title' in col]+[col for col in df.columns if 'Title' in col]\n",
    "    #doi_labels = [col for col in df.columns if 'doi' in col]+[col for col in df.columns if 'DOI' in col]+[col for col in df.columns if 'Doi' in col]\n",
    "    \n",
    "    #difflib.get_close_matches('Author_Name', author_cols)\n",
    "    #difflib.get_close_matches('Title', title_cols)\n",
    "    \n",
    "    \n",
    "    \"\"\"After matching, RAISING ERRORS for incorrect input, and then do edge test in testing\"\"\"  \n",
    "    # get keywords in the article title to remove useless results\n",
    "    keyword_list = key_words(df)\n",
    "    keyword_list = (list(zip(*keyword_list))[0])\n",
    "    \n",
    "    # scrap by author name\n",
    "    scraping_author_df = pd.DataFrame(scraping_author(df))\n",
    "    key_count = [0]*len(scraping_author_df)\n",
    "    for i in range(len(scraping_author_df)):\n",
    "        for j in range(len(keyword_list)):\n",
    "            if keyword_list[j] in scraping_author_df.iloc[i].loc['news_title']:\n",
    "                key_count[i] = key_count[i]+1\n",
    "    \n",
    "    # know how many key words in each result when scraping by author name\n",
    "    scraping_author_df['key_count'] = key_count\n",
    "    \n",
    "    # scrap by doi\n",
    "    scraping_doi_df = pd.DataFrame(scraping_doi(df))\n",
    "    \n",
    "    # scrap by title\n",
    "    scraping_title_df = pd.DataFrame(scraping_title(df)) \n",
    "    \n",
    "    # concat all df's together\n",
    "    lst = [scraping_author_df, scraping_doi_df, scraping_title_df]  # List of your dataframes\n",
    "    df_result= pd.concat(lst)\n",
    "    \n",
    "    # remove irrelevant and duplicate results\n",
    "    df_result = df_result[df_result['key_count'] != 0]\n",
    "    df_result.drop_duplicates(subset=['news_link'], inplace = True)\n",
    "\n",
    "    return df_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "633b9e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do:\n",
    "# Value error checking for header names\n",
    "# Write unit tests -- Ishika think of cases\n",
    "# Write documentation for new code\n",
    "# Update documentation for changes\n",
    "# Linking frontend and backend stuff\n",
    "# Repeat documention stuff for other use cases\n",
    "# Update design docs\n",
    "# Update package specs in environment\n",
    "# Continuous integration (WTF LEARN THIS)\n",
    "# Delete one use case from documentation and stick with only two use cases (Delaney doing 12/4/22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd600e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = df.dropna(how =\\'all\\') # drop last n rows because google news functions \\n#won\\'t work with NaN or NA values, we need to add an ifelse/judgement to each function. We only can search for strings\\ndf.dropna(how=\\'all\\', axis=1, inplace=True)\\ndf\\n\\nauthor_cols = [col for col in df.columns if \\'author\\' in col]+[col for col in df.columns if \\'Author\\' in col]\\n#print(list(df.columns))\\nprint(author_cols)\\ntitle_cols = [col for col in df.columns if \\'title\\' in col]+[col for col in df.columns if \\'Title\\' in col]\\nprint(title_cols)\\ndoi_labels = [col for col in df.columns if \\'doi\\' in col]+[col for col in df.columns if \\'DOI\\' in col]+[col for col in df.columns if \\'Doi\\' in col]\\nprint(doi_labels)\\n\\n\\n# write a function where we get indices of headers which contain the string \"Author\", \"DOI\" and \"Article Title\"\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df = df.dropna(how ='all') # drop last n rows because google news functions \n",
    "#won't work with NaN or NA values, we need to add an ifelse/judgement to each function. We only can search for strings\n",
    "df.dropna(how='all', axis=1, inplace=True)\n",
    "df\n",
    "\n",
    "author_cols = [col for col in df.columns if 'author' in col]+[col for col in df.columns if 'Author' in col]\n",
    "#print(list(df.columns))\n",
    "print(author_cols)\n",
    "title_cols = [col for col in df.columns if 'title' in col]+[col for col in df.columns if 'Title' in col]\n",
    "print(title_cols)\n",
    "doi_labels = [col for col in df.columns if 'doi' in col]+[col for col in df.columns if 'DOI' in col]+[col for col in df.columns if 'Doi' in col]\n",
    "print(doi_labels)\n",
    "\n",
    "\n",
    "# write a function where we get indices of headers which contain the string \"Author\", \"DOI\" and \"Article Title\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "39c237c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport difflib as difflib\\ndifflib.get_close_matches('Author_Name', author_cols)\\ndifflib.get_close_matches('Title', title_cols)\\n\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import difflib as difflib\n",
    "difflib.get_close_matches('Author_Name', author_cols)\n",
    "difflib.get_close_matches('Title', title_cols)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11d6779",
   "metadata": {},
   "source": [
    "### Use keywords in peer-reviewed article titles to reduce irrelevant return items from the scraping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e718e1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nkeyword_list = key_words(df)\\nkeyword_list = (list(zip(*keyword_list))[0])\\nprint(keyword_list)\\n#keywords(df)\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def key_words(df):\n",
    "    article_titles = df['Article_Title'].to_list()\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punct = set(string.punctuation)\n",
    "    \n",
    "    for i in range(len(article_titles)):\n",
    "        # lowercasing the text\n",
    "        article_titles[i] = str(article_titles[i]).lower()\n",
    "        # remove stopwords\n",
    "        article_titles[i] = \" \".join([word for word in article_titles[i].split() if word not in stop_words])\n",
    "        # remove unicode\n",
    "        text = article_titles[i].encode(encoding='ascii', errors='ignore').decode()\n",
    "        article_titles[i] = \" \".join([word for word in text.split()])\n",
    "        # remove market ticker and hashtag\n",
    "        article_titles[i] = re.sub('\\$', '', article_titles[i])\n",
    "        article_titles[i] = re.sub('\\#', '', article_titles[i])\n",
    "        # remove punctuation\n",
    "        article_titles[i] = \"\".join([ch for ch in article_titles[i] if ch not in punct])\n",
    "        \n",
    "    res = ' '.join(article_titles)\n",
    "    res = res.split(' ')\n",
    "    return Counter(res).most_common(10)\n",
    "\n",
    "'''\n",
    "keyword_list = key_words(df)\n",
    "keyword_list = (list(zip(*keyword_list))[0])\n",
    "print(keyword_list)\n",
    "#keywords(df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb2fe4",
   "metadata": {},
   "source": [
    "### Function for scraping by author name only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c66004e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nscraping_author_df = pd.DataFrame(scraping_author(df))# this puts the output in a long form dataframe/\\n#scraping_author_df \\nkey_count = [0]*len(scraping_author_df)\\nfor i in range(len(scraping_author_df)):\\n    for j in range(len(keyword_list)):\\n        #print(keyword_list[j])\\n        if keyword_list[j] in scraping_author_df.iloc[i].loc['news_title']:\\n            key_count[i] = key_count[i]+1\\n\\nscraping_author_df['key_count'] = key_count\\n\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search = gn.search(‘author name’) #search by author name\n",
    "def scraping_author(df):\n",
    "    \"\"\"This function scrapes google news for anything matching the \n",
    "    scholarly author name in df\"\"\"\n",
    "    #df_author = df.dropna([df.iloc'Author_Name'])\n",
    "    df_author = df[df.Author_Name.notnull()]\n",
    "\n",
    "    stories = []\n",
    "    for i in range(len(df_author)):\n",
    "        search = gn.search(df_author.iloc[i].loc['Author_Name'])\n",
    "        newsitem = search['entries']\n",
    "        for item in newsitem:\n",
    "            story = {\n",
    "                'Author_Name': df_author.iloc[i].loc['Author_Name'],\n",
    "                'Article_Title': df_author.iloc[i].loc['Article_Title'],\n",
    "                'Article_DOI': df_author.iloc[i].loc['Article_DOI'],\n",
    "                'news_title': item.title, #change to 'news_title' for clarity\n",
    "                'news_link': item.link\n",
    "            }\n",
    "            \n",
    "            stories.append(story)    \n",
    "    return stories\n",
    "\n",
    "'''\n",
    "scraping_author_df = pd.DataFrame(scraping_author(df))# this puts the output in a long form dataframe/\n",
    "#scraping_author_df \n",
    "key_count = [0]*len(scraping_author_df)\n",
    "for i in range(len(scraping_author_df)):\n",
    "    for j in range(len(keyword_list)):\n",
    "        #print(keyword_list[j])\n",
    "        if keyword_list[j] in scraping_author_df.iloc[i].loc['news_title']:\n",
    "            key_count[i] = key_count[i]+1\n",
    "\n",
    "scraping_author_df['key_count'] = key_count\n",
    "'''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fec4d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping_author_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b924cd",
   "metadata": {},
   "source": [
    "### Function for scraping by DOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8079a7ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nscraping_doi_df = pd.DataFrame(scraping_doi(df)) # this puts the output in a long form dataframe\\n\\nscraping_doi_df \\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search = gn.search(‘Article_DOI’) #search by reference to article DOI\n",
    "\n",
    "def scraping_doi(df):\n",
    "    '''This function scrapes google news for anything matching the \n",
    "    scholarly DOI in df'''\n",
    "    stories = []\n",
    "    df_doi = df[df.Article_DOI.notnull()]\n",
    "    for i in range(len(df_doi)):\n",
    "        search = gn.search(df_doi.iloc[i].loc['Article_DOI'])\n",
    "        newsitem = search['entries']\n",
    "        for item in newsitem:\n",
    "            story = {\n",
    "                'Author_Name': df_doi.iloc[i].loc['Author_Name'],\n",
    "                'Article_Title': df_doi.iloc[i].loc['Article_Title'],\n",
    "                'Article_DOI': df_doi.iloc[i].loc['Article_DOI'],\n",
    "                'news_title': item.title, #change to 'news_title' for clarity\n",
    "                'news_link': item.link\n",
    "            }\n",
    "            \n",
    "            stories.append(story)    \n",
    "    return stories\n",
    "'''\n",
    "scraping_doi_df = pd.DataFrame(scraping_doi(df)) # this puts the output in a long form dataframe\n",
    "\n",
    "scraping_doi_df \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa04ca00",
   "metadata": {},
   "source": [
    "### Function for scraping by exact article title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fd28c913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nscraping_title_df = pd.DataFrame(scraping_title(df)) \\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search = gn.search(‘Article_Title’) #search by reference to peer reviewed article title\n",
    "def scraping_title(df):\n",
    "    stories = []\n",
    "    df_article_title = df[df.Article_Title.notnull()]\n",
    "    for i in range(len(df_article_title)):\n",
    "        search = gn.search(df_article_title.iloc[i].loc['Article_Title'])\n",
    "        newsitem = search['entries']\n",
    "        for item in newsitem:\n",
    "            story = {\n",
    "                'Author_Name': df_article_title.iloc[i].loc['Author_Name'],\n",
    "                'Article_Title': df_article_title.iloc[i].loc['Article_Title'],\n",
    "                'Article_DOI': df_article_title.iloc[i].loc['Article_DOI'],\n",
    "                'news_title': item.title, #change to 'news_title' for clarity\n",
    "                'news_link': item.link\n",
    "            }\n",
    "            \n",
    "            stories.append(story)    \n",
    "    return stories\n",
    "'''\n",
    "scraping_title_df = pd.DataFrame(scraping_title(df)) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b32e51f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nlst = [scraping_author_df, scraping_doi_df, scraping_title_df]  # List of your dataframes\\ndf_result= pd.concat(lst)\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "lst = [scraping_author_df, scraping_doi_df, scraping_title_df]  # List of your dataframes\n",
    "df_result= pd.concat(lst)\n",
    "'''   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "46473dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd3c3088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nscrape_out.size\\nscrape_out.tail(1)\\nscrape_out = scrape_out[scrape_out.key_count != 0]\\n\\nscrape_out.head()\\nscrape_out.tail()\\nprint(scrape_out)\\n\\ndf_result = df_result[df_result['key_count'] != 0]\\n\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "scrape_out.size\n",
    "scrape_out.tail(1)\n",
    "scrape_out = scrape_out[scrape_out.key_count != 0]\n",
    "\n",
    "scrape_out.head()\n",
    "scrape_out.tail()\n",
    "print(scrape_out)\n",
    "\n",
    "df_result = df_result[df_result['key_count'] != 0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d63ba37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nscrape_out.drop_duplicates(subset=['news_link'])\\nscrape_out.head()\\nscrape_out.tail()\\nprint(scrape_out)\\n\\n\\ndf_result.drop_duplicates(subset=['news_link'], inplace = True)\\n\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "scrape_out.drop_duplicates(subset=['news_link'])\n",
    "scrape_out.head()\n",
    "scrape_out.tail()\n",
    "print(scrape_out)\n",
    "\n",
    "\n",
    "df_result.drop_duplicates(subset=['news_link'], inplace = True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bede4d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b0dd6840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chocoyao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#sample = {'a':[1, 2, 3],\n",
    "#          'b': ['a', 'b', 'c']}\n",
    "\n",
    "# Sample output file, need to change to our output file.\n",
    "output_df = main(df)\n",
    "\n",
    "# Display a download button widget for user to get the output file.\n",
    "st.download_button(\n",
    "    label=\"Download data as CSV\",\n",
    "    data=output_df.to_csv().encode('utf-8'),\n",
    "    file_name='news.csv',\n",
    "    mime='text/csv',\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80b87727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 15:42:41.221 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/anaconda3/lib/python3.8/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "def _max_width_():\n",
    "    \"\"\"\n",
    "    Display string formatted as Markdown.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_width_str = f\"max-width: 1800px;\"\n",
    "    st.markdown(\n",
    "        f\"\"\"\n",
    "    <style>\n",
    "    .reportview-container .main .block-container{{\n",
    "        {max_width_str}\n",
    "    }}\n",
    "    </style>    \n",
    "    \"\"\",\n",
    "        unsafe_allow_html=True,\n",
    "    )\n",
    "\n",
    "# Configures the default settings of the page.\n",
    "st.set_page_config(page_icon=\"🎓\", page_title=\"SCIPOP\")\n",
    "\n",
    "# Display an image with its url.\n",
    "st.image(\n",
    "    \"https://png.pngitem.com/pimgs/s/202-2021802_graduation-cap-emoji-transparent-hd-png-download.png\",\n",
    "    width=100,\n",
    ")\n",
    "\n",
    "# Draw Markdown-formatted text, with input as a string.\n",
    "st.write(\n",
    "\"\"\"\n",
    "# SCIPOP APP\n",
    "Upload your article dataset to see the relevant news.\\n\n",
    "#### Dataset guidance:\\n \n",
    "Column_Name: Author_Name, Article_Title, Article_DOI. Could see example dataset for reference.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Display a file uploader widget. Users can drag/drop their input file.\n",
    "uploaded_file = st.file_uploader(\"Upload CSV\", type=\".csv\")\n",
    "\n",
    "# Checkbox of example file to demo the app.\n",
    "use_example_file = st.checkbox(\n",
    "    \"Use example file\", False, help=\"Use in-built example file to demo the app\"\n",
    ")\n",
    "\n",
    "# Path for example file.\n",
    "if use_example_file:\n",
    "    uploaded_file = \"UseCase1_Data.csv\"\n",
    "    \n",
    "# Read and preview input file.    \n",
    "if uploaded_file:\n",
    "    df = pd.read_csv(uploaded_file)\n",
    "\n",
    "    st.markdown(\"### Data preview\")\n",
    "    st.dataframe(df.head())\n",
    "    \n",
    "\n",
    "    \n",
    "output_df = main(df)\n",
    "\n",
    "# Display a download button widget for user to get the output file.\n",
    "st.download_button(\n",
    "    label=\"Download data as CSV\",\n",
    "    data=output_df.to_csv().encode('utf-8'),\n",
    "    file_name='news.csv',\n",
    "    mime='text/csv',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba30542a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d85e38d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
